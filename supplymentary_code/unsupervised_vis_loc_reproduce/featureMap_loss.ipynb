{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\28340\\anaconda3\\envs\\fyp\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\28340\\anaconda3\\envs\\fyp\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image \n",
    "from torchvision import datasets, models, transforms\n",
    "import math \n",
    "import random \n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import imageio\n",
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.geometry as KG\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from numpy.linalg import inv\n",
    "from numpy.matlib import repmat\n",
    "import torchvision.models as models\n",
    "from dataset import *\n",
    "\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "base_dir = 'code/unspervised_vis_loc_reproduce/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.avgpool = torch.nn.Identity()\n",
    "vgg.features[30] = torch.nn.Identity()\n",
    "for param in vgg.features:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsampling(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=0):\n",
    "        super(Upsampling, self).__init__()\n",
    "        # print(\"in_channels: \",in_channels)\n",
    "        # print(\"out_channels: \",out_channels)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.upsample = torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "                                     \n",
    "    def forward(self, x1,x2,final = False):\n",
    "        x1up = self.upsample(x1)\n",
    "        # print(\"x1up?: \",x1up.shape)\n",
    "        # print(\"x2.shape: \",x2.shape)\n",
    "        if x1up.shape != x2.shape:\n",
    "            x1up = torch.nn.functional.pad(x1up, (0, x2.shape[3] - x1up.shape[3], 0, x2.shape[2] - x1up.shape[2]))\n",
    "\n",
    "        x = torch.cat([x2, x1up], dim=1)\n",
    "        if final:\n",
    "            output= torch.nn.Sequential(torch.nn.Conv2d(64+17, 17, kernel_size= 2, padding= 0,stride=1),\n",
    "                                     torch.nn.BatchNorm2d(17),\n",
    "                                     torch.nn.ReLU(True),\n",
    "                                     \n",
    "                                     torch.nn.Conv2d(17,17, kernel_size=1, padding= 0,stride=1),\n",
    "                                     torch.nn.Sigmoid())(x)\n",
    "            return output\n",
    "        output = torch.nn.Sequential(torch.nn.Conv2d(self.in_channels, self.out_channels, kernel_size= 2, padding= 0,stride=1),\n",
    "                                     torch.nn.BatchNorm2d(self.out_channels),\n",
    "                                     torch.nn.ReLU(True),\n",
    "                                     torch.nn.Conv2d(self.out_channels, self.out_channels, kernel_size= 2, padding= 0,stride=1),\n",
    "                                     torch.nn.BatchNorm2d(self.out_channels),\n",
    "                                     torch.nn.ReLU(True),\n",
    "                                     torch.nn.Conv2d(self.out_channels, self.out_channels, kernel_size= 2, padding= 0,stride=1),\n",
    "                                     torch.nn.BatchNorm2d(self.out_channels),\n",
    "                                     torch.nn.ReLU(True))(x)\n",
    "        \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.vgg = vgg\n",
    "\n",
    "        self.up_1 = Upsampling(1024, 17)\n",
    "        self.up_2 = Upsampling(256+17, 17)\n",
    "        self.up_3 = Upsampling(128+17, 17)\n",
    "        self.up_4 = Upsampling(64+17, 17)\n",
    "        # self.up_4 =\n",
    "        # self.outputs = LastConv(64+17, 17)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x1 = self.vgg.features[0:4](x)\n",
    "        x2 = self.vgg.features[4:9](x1)\n",
    "        x3 = self.vgg.features[9:16](x2)\n",
    "        x4 = self.vgg.features[16:23](x3)\n",
    "        x5 = self.vgg.features[23:30](x4)\n",
    "\n",
    "\n",
    "        x6 = self.up_1(x5,x4)\n",
    "        x7 = self.up_2(x6,x3)\n",
    "        x8 = self.up_3(x7,x2)\n",
    "        x9 = self.up_4(x8,x1,True)\n",
    "\n",
    "        # (F3,S3),(F2,S2),(F1,S1),(F0,S0)\n",
    "        return (x6[:15],x6[15:]),(x7[:15],x7[15:]),(x8[:15],x8[16:]),(x9[:15],x9[16:])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureMap = UNet(3,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Carla_Dataset(img_dir='carla_npzs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\28340\\Documents\\UCL\\finalYearProject\\code\\unsupervised_vis_loc_reproduce\\featureMap_loss.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/28340/Documents/UCL/finalYearProject/code/unsupervised_vis_loc_reproduce/featureMap_loss.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m (F3,S3),(F2,S2),(F1,S1),(F0,S0) \u001b[39m=\u001b[39m featureMap(dataset[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "(F3,S3),(F2,S2),(F1,S1),(F0,S0) = featureMap(dataset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92e853dbaceed3088030898fb8081c1c7a38fe0d9e4ab9e36a77882e6958d054"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
